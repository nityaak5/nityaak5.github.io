---
title: "Explainability vs. Interpretability: A Tale of Two Questions"
date: 2025-01-07
categories:
  - blog 
  - tech
tags:
  - explainable AI
  - interpretability
---

<p>Original Medium post: <a href="https://medium.com/@nityaakalra5/explainability-vs-interpretability-a-tale-of-two-questions-c09b8490a399" target="_blank" rel="noopener">Explainability vs. Interpretability: A Tale of Two Questions</a>.</p>

<figure><img alt="Source: https://cyara.com/blog/best-practices-for-automated-conversational-ai-testing-in-2024/" src="https://cdn-images-1.medium.com/max/1024/1*RK0V7gAmrloE2y80mLMneA.jpeg" /></figure>

<p>A few weeks ago, I stumbled upon <a href="https://www.neelnanda.io/blog/27-retrospective">Neel Nanda</a>’s blog, where he talked about embracing imperfection in writing. He decided to publish raw, unpolished drafts, letting his inner thoughts spill onto the page. Inspired by that, I thought, why not try the same? Here I am, experimenting with publishing first drafts, raw thoughts, and minimal polish. Let’s see where this goes.</p><p>Enough of this rambling, let’s get back to the topic. 

For quite some time now, I have been interested in how we can build AI systems responsibly. Of course, the most obvious solution would be to make them more explainable and interpretable. Yet, distinguishing between these two terms can be tricky sometimes, even for experts. Quite a few papers and books even use the terms interchangeably, which honestly, seems reasonable. But when we’re talking about a specific system- say, a medical diagnostic model, what should we prioritise between the two? This is where the need to differentiate between them arises.</p><p>A simple way to differentiate between the two is by answering the <em>‘Why’ </em>and the <em>‘How’</em>, as is clear from this <a href="https://www.splunk.com/en_us/blog/learn/explainability-vs-interpretability.html">post</a>.</p>

<p><strong>Explainability</strong> answers why a model made a specific decision. For example, let’s say a Named Entity Recognition (NER) model tags <em>“Paris”</em> as a location. Tools like <a href="https://arxiv.org/abs/1602.04938">LIME</a> or <a href="https://arxiv.org/abs/1705.07874">SHAP</a> can highlight the words, such as <em>“capital”</em> or <em>“France” </em>that influenced this decision. <strong><em>This is explainability. It provides post-hoc insights into the model’s output.</em></strong></p><p><strong>Interpretability</strong>, on the other hand focuses on how the model reached its decision. For a Transformer-based model like <a href="https://arxiv.org/abs/1810.04805">BERT</a>, this might involve analysing attention weights to see which words the model prioritised. Or it could mean visualising how hidden layers process and transform <em>“Paris” </em>into a vector that the model recognises as a location. Basically, <strong><em>interpretability digs into the model’s internal mechanics.</em></strong></p><p>But then one might think, is an explainable system always interpretable? Well, not necessarily. A model can explain its decisions without us fully understanding its internal workings. And here’s the kicker: even if we understand the how a decision was made by a model, it doesn’t guarantee we’ll agree with why that decision was made. And this is one of the things that feels like an endless rabbit hole.</p><p>Maybe we can simplify it a bit more. Let’s start with this analogy:</p>

<blockquote>Imagine you come across a safe with a digital lock. You punch in a random code, and voila! It opens. You’re amazed but also curious: how did this happen?</blockquote><blockquote><strong>Explainability</strong>: Someone says, “The safe opened because the code you entered matched a pre-set combination.” This explains why it opened.</blockquote><blockquote><strong>Interpretability</strong>: Now, suppose that same person opens the safe and shows you the inner workings. The gears, sensors, and electronics that verified your code and unlocked the mechanism. This reveals how the safe operates.</blockquote><p>See the difference? <strong><em>Explainability gives you the reasoning; interpretability shows you the process.</em></strong> I admit, I still get confused sometimes. But this example helps me keep things somewhat straight.</p><h3><strong>Now, Why Do We Care?</strong></h3><p>Understanding the distinction between explainability and interpretability is crucial when designing AI systems, especially in high-stakes domains like healthcare, finance, or criminal justice. An explainable system can provide users with the rationale behind decisions, fostering trust. Meanwhile, an interpretable system ensures developers and regulators understand the underlying mechanics, enabling more effective debugging and bias mitigation. Both aspects play pivotal roles in building responsible AI.</p><h3><strong>Final Thoughts</strong></h3><p>We could go on and on about why we need to make our models more interpretable and explainable. One simple reason- as Christopher Molnar explains is, human mind is naturally curious. We want to understand why things happen, especially when they are unexpected. Why did the dog bite me, even though it has never shown aggression before? Why was my loan application rejected? For an AI system to be trusted by humans, even a modest explanation of its behaviour is somewhat essential.</p><p>But for now, we’ll stop here and save that for another post. If you’re curious in the meantime, I highly recommend referring to Molnar’s book- <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a>. Though it was originally published in 2019, it is still highly relevant today.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c09b8490a399" width="1" height="1" alt="">
