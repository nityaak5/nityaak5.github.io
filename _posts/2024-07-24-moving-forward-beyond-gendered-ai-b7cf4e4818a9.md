---
title: "Moving Forward: Beyond Gendered AI"
date: 2024-07-24
categories:
  - blog
  - research
tags:
  - gender bias
  - voice assistants
---

<p>Original Medium post: <a href="https://medium.com/@nityaakalra5/moving-forward-beyond-gendered-ai-b7cf4e4818a9" target="_blank" rel="noopener">Moving Forward: Beyond Gendered AI</a>.</p>

<p>In this final article, we explore recent advancements in AI voice assistants that transcend the traditional male/female dichotomy. This evolution aims to provide more inclusive and diverse options, but it raises important questions. Is simply adding more voices enough? Some recent attempts at gender-neutral voices have not been as successful as hoped. Understanding why these approaches have fallen short is crucial to making meaningful progress and this article sheds some light on the same. After all, the key to finding effective solutions lies in fully understanding the problem.</p><p>To begin, let’s examine how we as humans, perceive and interact with these digital voices.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*YJHS0t70FlHqq9tGrleWUQ.jpeg" /><figcaption>Source: <a href="https://www.cartoonsidrew.com/2015/08/siri-says.html">https://www.cartoonsidrew.com/2015/08/siri-says.html</a></figcaption></figure><h3>Anthropomorphization and User Perception</h3><p>Have you ever noticed how we tend to give human traits to our gadgets? This tendency, known as <em>Anthropomorphization</em>, is something most of us do without thinking. The<em> Computers as Social Actors (CASA) </em>paradigm, developed by Clifford Nass, explains this behaviour. We often assign a gender to our technology based on subtle cues like voice tone or communication style, even when the technology isn’t explicitly gendered. This inclination, shaped by societal norms, makes it incredibly challenging to eliminate gender biases in AI voice assistants.</p><p>Let’s say you start talking to your voice assistant as if it were a new ‘female’ assistant in your office. You might instinctively use the pronouns ‘she/her’ to refer to this piece of technology, inadvertently reinforcing some typical stereotypes associated with women. This humanising of AI plays a significant role in dehumanising women — a concept thoroughly explained by <a href="https://osf.io/preprints/socarxiv/jqxb6"><em>Pygmalion Displacement.</em></a> This theory posits that when we frequently associate female voices with subordinate roles like assistants, we risk reinforcing the stereotype that the ideal voice of authority is the one devoid of feminine qualities. Understanding this dynamic is crucial in our efforts to create AI systems that truly inclusive and unbiased.</p><h3>The Roadblocks to a Truly Gender Neutral Voice Assistant</h3><p>Recent years have witnessed a concerted effort to dismantle gender bias in these voice assistants. For instance, Apple’s Siri underwent changes to offer multiple voice options, and Google adopted a gender-neutral name to <a href="https://www.adaptworldwide.com/insights/2021/gender-bias-in-ai-why-voice-assistants-are-female">symbolise its inclusivit</a>y.</p><p>However, despite these advancements, significant challenges remain. For instance, when Google introduced its Assistant in 2016, it was primarily trained using data from female voices. This resulted in better performance with female voices only and thereby discarding the idea to launch both male and female voiced assistant initially.</p><p>Studies show that users tend to<a href="https://dl.acm.org/doi/abs/10.1145/3411763.3451623"> trust both male and female AI voices equally,</a> suggesting that gender neutrality might be a viable path to reducing bias. But is it really that simple? <a href="https://www.npr.org/2019/03/21/705395100/meet-q-the-gender-neutral-voice-assistant">The rise and fall of <em>‘Q</em></a><em>’,</em> the gender-neutral voice assistant, serves as a cautionary tale. Despite its ambitious goal of inclusivity, <em>‘Q’ </em>struggled to resonate with users. This stark reality underscores the complexities of achieving true gender equality in AI and suggests that while gender neutrality is a step forward, it may not be sufficient on its own.</p><h3>Moving Towards Inclusive Design</h3><p>Drawing inspiration from the concept of <a href="https://arxiv.org/abs/2302.10893"><em>‘Fair Diffusion’</em></a>, which allows users to control AI image synthesis, we can envision similar strategies for voice design. Imagine a system where users can personalise their AI assistant’s voice by adjusting pitch, accent, and communication style, creating a truly customised and inclusive experience. This approach holds promise for enhancing user engagement, but it also presents potential pitfalls.</p><p>For instance, the emphasis on extensive customisation might lead users to spend excessive time tweaking their assistant’s voice, possibly overshadowing its core functionality. Further, it might play on the same roads of existing stereotypes. Users might gravitate towards the same <em>‘high-pitched,’ ‘soft-spoken,’ </em>and <em>‘calming’ </em>voices, giving into these biases rather than challenging them.</p><p>In <a href="https://www.researchgate.net/publication/342599797_Gender_Ambiguous_not_Genderless_Designing_Gender_in_Voice_User_Interfaces_VUIs_with_sensitivity_-preprint">her paper</a>, S. J. Sutton also suggests engaging with other gendering elements apart from just voice. She emphasises moving towards <em>‘gender ambiguity’</em> rather than <em>‘gender neutrality’ </em>to create a more refined framework for gender in AI design.</p><p>Another potential solution could involve expanding the training data to incorporate not only more female voices but also voices from other marginalised communities. This does not limit to different vocal textures and tones, but also different accents and articulation. But let’s be clear: more data alone isn’t a magic solution. While essential, it’s just one piece of the puzzle (I won’t bore you with the reasons here). Overcoming the deep-rooted biases in AI requires a more comprehensive approach such as ensuring representation from marginalised groups during the development of these technologies.</p><p>As we move forward, the critical question remains: Are we ready to confront the power structures that perpetuate bias in AI design? The response will influence the future of human-AI interactions, as well as the ideals underlying these technologies. By proactively promoting inclusivity, we can ensure that AI assistants are a catalyst for positive social change, not a reflection of past inequalities :)</p><h3>References</h3><p>[1] C. Nass and Y. Moon, “Machines and mindlessness: Social responses to computers,” J. Soc. Issues, vol. 56, no. 1, pp. 81–103, 2000.</p><p>[2] L. Erscoi, A. V. Kleinherenbrink, and O. Guest, “Pygmalion displacement: When humanising AI dehumanises women,” SocArXiv, 2023.</p><p>[3] E. Fisher, “Gender bias in AI: Why voice assistants are female,” Adapt. [Online]. Available: <a href="https://www.adaptworldwide.com/insights/2021/gender-bias-in-ai-why-voice-assistants-are-female">https://www.adaptworldwide.com/insights/2021/gender-bias-in-ai-why-voice-assistants-are-female</a>.</p><p>[4] Meet Q: The First Genderless Voice- FULL SPEECH. 2019.</p><p>[5] F. Friedrich, Fair diffusion: Instructing text-to-image generation models on fairness. 2023.</p><p>[6] S. J. Sutton, “Gender ambiguous, not genderless: Designing gender in voice user interfaces (VUIs) with sensitivity,” in <em>Proceedings of the 2nd Conference on Con</em>versational User Interfaces, 2020.</p><p>[7] <em>Female by Default? — Exploring the Effect of Voice Assistant Gender and Pitch on Trait and Trust Attribution</em>. .</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b7cf4e4818a9" width="1" height="1" alt="">
