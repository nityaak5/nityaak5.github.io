---
title: "Outsourcing Morality"
date: 2025-10-18
categories:
  - thinking-aloud
  - research

tags:
  - ai ethics
---

<p>Original Medium post: <a href="https://medium.com/@nityaakalra5/outsourcing-morality-fe975e317268" target="_blank" rel="noopener">Outsourcing Morality</a>.</p>

<h4><em>How convenience has become the most convincing excuse in the age of AI.</em></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/420/1*jN6-JsRL3_bx_XFm-_7GSA.jpeg" /><figcaption>Source: <a href="https://upstudio.medium.com/the-art-of-ethical-designs-684d71668e7f">https://upstudio.medium.com/the-art-of-ethical-designs-684d71668e7f</a></figcaption></figure><p>You open your laptop, with a deadline blinking on the screen. Out of habit, you open your favourite AI tool and ask it to <em>“make this sound better.”</em> It rewrites three perfect paragraphs for you. You copy, tweak, and hit submit.</p><p>You didn’t cheat, right? You just made things… easier. This comfortable justification is the core of a new dilemma.</p><p>A friend of mine recently learned this lesson the hard way. He submitted an assignment that still had the citation section unfilled. He hadn’t used AI at all, but because the formatting looked suspiciously ‘ChatGPT-ish,’ his professor immediately accused him of using AI to generate fake references. He spent days defending something he hadn’t done.</p><p>That moment stuck with me. The issue was not simply cheating; it had become a crisis of trust. In a world saturated with AI tools, the boundary between honest effort and automated aid is blurring so quickly that even good intentions can look guilty. And yet, research (see below) suggests that when we <em>do</em> cheat using AI, we often don’t even feel dishonest.</p><h3>When Delegating Feels Harmless</h3><p>In a recent <a href="https://www.nature.com/articles/s41586-025-09505-x"><em>Nature</em> study</a>, Nils Köbis and colleagues showed how AI changes the way humans make ethical choices. The experiment presented participants with tasks where dishonesty could bring material gain, such as reporting higher dice rolls or tweaking tax numbers.</p><p>When people were faced with the direct choice to report a lie (like reporting a higher dice roll), <strong>most of them refused</strong>. Perhaps their internal moral alarm sounded, and they chose integrity over profit.</p><p>But the moment they could simply <strong>delegate </strong>the task to an AI agent with instructions to ‘maximise the profit,’ dishonesty rose dramatically. The AI models almost always cheated or pursued the most effective (though unethical) path to the goal. And what happened? People didn’t feel like <em>they</em> were lying; they were simply <em>“letting the machine handle it.” </em>This psychological distancing creates what we might call moral anaesthesia.</p><p>While developers work tirelessly to install safety filters that prevent explicit harms (like generating hate speech), these systems are insufficient for navigating subtle ethical scenarios. As <a href="https://arxiv.org/abs/2411.14442">Šekrst et al. </a>argued in <em>AI Ethics by Design</em>, current safeguards create an illusion of control. <strong>They handle bad words, not bad intentions.</strong></p><h3>A Crisis of Trust</h3><p>The consequences of this moral delegation are not just psychological; they are tearing at the fabric of trust in our institutions, especially in education.</p><p>A study on <a href="https://www.emerald.com/jarhe/article-abstract/doi/10.1108/JARHE-10-2024-0522/1257798/Navigating-the-ethical-dilemma-in-digital-learning?redirectedFrom=fulltext">academic dishonesty </a>in digital learning found that students were generally <strong>reluctant to admit the use of AI </strong>when asked directly, largely due to shame and social pressure. Instead, when a more anonymous method was used, the admission rate for academic dishonesty shot up significantly. The tragedy here is that the mere suspicion of AI use is enough to shatter trust, even for the innocent.</p><p>Both studies, one focusing on psychology and one on education, point to the same truth: <strong>when responsibility is diluted, morality becomes negotiable. </strong>We have built a system that cannot tell right from wrong, and in using it, we have started to compromise the standards by which we judge integrity.</p><h3>A Human Question</h3><p>The true dilemma of the AI era is the choice we face every day: to lean into the perfect <strong>convenience</strong> offered by the machine, or to endure the necessary discomfort of <strong>conscious</strong> decision-making.</p><p>Let’s be honest: I take shortcuts too. You take shortcuts. Finding the path of least resistance or greatest efficiency is often what drives innovation and productivity. But then, there is a fine line between legitimate efficiency and convenient integrity bypass. That line is crossed when we delegate a task we <em>know</em> is ethically questionable, simply to avoid the personal moral friction.</p><p>When faced with the direct choice to cheat, our morals and our self-image kick in. We hesitate. We pause. Perhaps this very struggle, <strong>the tension between convenience and conscience, might be the most human thing after all.</strong> And in a strange way, isn’t that struggle beautiful?</p><h3>References</h3><p>N. Köbis <em>et al.</em>, “Delegation to artificial intelligence can increase dishonest behaviour,” <em>Nature</em>, vol. 646, no. 8083, pp. 126–134, 2025.</p><p>K. Šekrst, J. McHugh, and J. R. Cefalu, “AI ethics by design: Implementing customizable guardrails for responsible AI development,” <em>arXiv [cs.CY]</em>, 2024.</p><p>E. Pariyanti, M. Wibowo, Z. Sultan, and S. P. Siolemba Patiro, “Navigating the ethical dilemma in digital learning: balancing artificial intelligence with spiritual integrity to address cheating,” <em>J. Appl. Res. High. Educ.</em>, 2025.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fe975e317268" width="1" height="1" alt="">
